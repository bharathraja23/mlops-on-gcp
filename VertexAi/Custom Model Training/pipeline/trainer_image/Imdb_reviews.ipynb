{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9ddef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from os import system, listdir\n",
    "from os.path import isfile, join\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "def create_data_frame(folder: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    folder - the root folder of train or test dataset\n",
    "    Returns: a DataFrame with the combined data from the input folder\n",
    "    '''\n",
    "    pos_folder = f'{folder}/pos' # positive reviews\n",
    "    neg_folder = f'{folder}/neg' # negative reviews\n",
    "    \n",
    "    def get_files(fld: str) -> list:\n",
    "        '''\n",
    "        fld - positive or negative reviews folder\n",
    "        Returns: a list with all files in input folder\n",
    "        '''\n",
    "        return [join(fld, f) for f in listdir(fld) if isfile(join(fld, f))]\n",
    "    \n",
    "    def append_files_data(data_list: list, files: list, label: int) -> None:\n",
    "        '''\n",
    "        Appends to 'data_list' tuples of form (file content, label)\n",
    "        for each file in 'files' input list\n",
    "        '''\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'r') as f:\n",
    "                text = f.read()\n",
    "                data_list.append((text, label))\n",
    "    \n",
    "    pos_files = get_files(pos_folder)\n",
    "    neg_files = get_files(neg_folder)\n",
    "    \n",
    "    data_list = []\n",
    "    append_files_data(data_list, pos_files, 1)\n",
    "    append_files_data(data_list, neg_files, 0)\n",
    "    shuffle(data_list)\n",
    "    \n",
    "    text, label = tuple(zip(*data_list))\n",
    "    # replacing line breaks with spaces\n",
    "    text = list(map(lambda txt: re.sub('(<br\\s*/?>)+', ' ', txt), text))\n",
    "    \n",
    "    return pd.DataFrame({'text': text, 'label': label})\n",
    "\n",
    "imdb_train = create_data_frame('../aclImdb/train')\n",
    "imdb_test = create_data_frame('../aclImdb/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72493993",
   "metadata": {},
   "outputs": [],
   "source": [
    "system(\"mkdir 'csv'\")\n",
    "imdb_train.to_csv('csv/imdb_train.csv', index=False)\n",
    "imdb_test.to_csv('csv/imdb_test.csv', index=False)\n",
    "\n",
    "# imdb_train = pd.read_csv('csv/imdb_train.csv')\n",
    "# imdb_test = pd.read_csv('csv/imdb_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c4e8e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./pipeline/trainer_image/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./pipeline/trainer_image/train.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import system\n",
    "import subprocess\n",
    "import sys\n",
    "import fire\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from joblib import dump, load # used for saving and loading sklearn objects\n",
    "from scipy.sparse import save_npz, load_npz, csr_matrix # used for saving and loading sparse matrices\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "def train_model(training_dataset_file, validation_dataset_file, gcs_model_path):\n",
    "    \n",
    "    imdb_train = pd.read_csv(training_dataset_file)\n",
    "    imdb_test = pd.read_csv(validation_dataset_file)\n",
    "    \n",
    "    if not os.path.exists('data_preprocessors'):\n",
    "        system(\"mkdir 'data_preprocessors'\")\n",
    "    if not os.path.exists('vectorized_data'):\n",
    "        system(\"mkdir 'vectorized_data'\")\n",
    "    if not os.path.exists('model'):\n",
    "        system(\"mkdir 'model'\")\n",
    "\n",
    "    #preprocessing\n",
    "\n",
    "    # Bigram Counts\n",
    "\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    bigram_vectorizer.fit(imdb_train['text'].values)\n",
    "    \n",
    "    dump(bigram_vectorizer, 'data_preprocessors/bigram_vectorizer.joblib')\n",
    "    \n",
    "    # bigram_vectorizer = load('data_preprocessors/bigram_vectorizer.joblib')\n",
    "    \n",
    "    X_train_bigram = bigram_vectorizer.transform(imdb_train['text'].values)\n",
    "    \n",
    "    save_npz('vectorized_data/X_train_bigram.npz', X_train_bigram)\n",
    "    \n",
    "    # X_train_bigram = load_npz('vectorized_data/X_train_bigram.npz')\n",
    "    \n",
    "    # Bigram Tf-Idf\n",
    "\n",
    "    bigram_tf_idf_transformer = TfidfTransformer()\n",
    "    bigram_tf_idf_transformer.fit(X_train_bigram)\n",
    "\n",
    "    dump(bigram_tf_idf_transformer, 'data_preprocessors/bigram_tf_idf_transformer.joblib')\n",
    "\n",
    "    # bigram_tf_idf_transformer = load('data_preprocessors/bigram_tf_idf_transformer.joblib')\n",
    "\n",
    "    X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\n",
    "\n",
    "    save_npz('vectorized_data/X_train_bigram_tf_idf.npz', X_train_bigram_tf_idf)\n",
    "    \n",
    "    # X_train_bigram_tf_idf = load_npz('vectorized_data/X_train_bigram_tf_idf.npz')\n",
    "    \n",
    "    y_train = imdb_train['label'].values\n",
    "\n",
    "    def train_and_show_scores(X: csr_matrix, y: np.array, title: str) -> None:\n",
    "        #splitting data\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=0.75, stratify=y\n",
    "        )\n",
    "\n",
    "        \n",
    "        clf = SGDClassifier()\n",
    "\n",
    "        distributions = dict(\n",
    "            loss=['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "            learning_rate=['optimal', 'invscaling', 'adaptive'],\n",
    "            eta0=uniform(loc=1e-7, scale=1e-2)\n",
    "        )\n",
    "\n",
    "        random_search_cv = RandomizedSearchCV(\n",
    "            estimator=clf,\n",
    "            param_distributions=distributions,\n",
    "            cv=5,\n",
    "            n_iter=20\n",
    "        )\n",
    "        random_search_cv.fit(X_train, y_train)\n",
    "        print(f'Best params: {random_search_cv.best_params_}')\n",
    "        print(f'Best score: {random_search_cv.best_score_}')\n",
    "        dump(random_search_cv.best_estimator_, 'model/model.joblib.pkl')\n",
    "        subprocess.check_call(['gsutil', 'cp', 'model/model.joblib.pkl', gcs_model_path],\n",
    "                        stderr=sys.stdout)\n",
    "        print('Saved model in: {}'.format(gcs_model_path))\n",
    "\n",
    "    train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "  fire.Fire(train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6731a3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Tf-Idf\n",
      "Train score: 0.98 ; Validation score: 0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import \n",
    "import numpy as np\n",
    "\n",
    "def train_and_show_scores(X: csr_matrix, y: np.array, title: str) -> None:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=0.75, stratify=y\n",
    "    )\n",
    "\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    valid_score = clf.score(X_valid, y_valid)\n",
    "    print(f'{title}\\nTrain score: {round(train_score, 2)} ; Validation score: {round(valid_score, 2)}\\n')\n",
    "\n",
    "y_train = imdb_train['label'].values\n",
    "\n",
    "\n",
    "train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7077bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://verexai_automl_text_data/custommodel/data/imdb_test.csv [Content-Type=text/csv]...\n",
      "Copying gs://verexai_automl_text_data/custommodel/data/imdb_train.csv [Content-Type=text/csv]...\n",
      "/ [2 files][ 61.8 MiB/ 61.8 MiB]                                                \n",
      "Operation completed over 2 objects/61.8 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://verexai_automl_text_data/custommodel/data/* gs://verexai_automl_text_data/pipeline_root/custommodel/imdb_reviews/data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52998946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/jupyter/Vertexai-Custom-Model-Covertype/Imdb_reviews_custom/csv/imdb_train.csv [Content-Type=text/csv]...\n",
      "- [1 files][ 31.2 MiB/ 31.2 MiB]                                                \n",
      "Operation completed over 1 objects/31.2 MiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp /home/jupyter/Vertexai-Custom-Model-Covertype/Imdb_reviews_custom/csv/imdb_train.csv gs://verexai_automl_text_data/custommodel/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fb71c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Tf-Idf\n",
      "Train score: 0.98 ; Validation score: 0.91\n",
      "\n",
      "Copying file://model/model.joblib.pkl [Content-Type=application/octet-stream]...\n",
      "/ [1 files][ 11.6 MiB/ 11.6 MiB]                                                \n",
      "Operation completed over 1 objects/11.6 MiB.                                     \n",
      "Saved model in: gs://verexai_automl_text_data/custommodel/model/\n"
     ]
    }
   ],
   "source": [
    "!python3 ./pipeline/trainer_image/train.py \"gs://verexai_automl_text_data/custommodel/data/imdb_train.csv\" \"gs://verexai_automl_text_data/custommodel/data/imdb_test.csv\" \"gs://verexai_automl_text_data/custommodel/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2d9e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='dna-verizonpoc'\n",
    "IMAGE_NAME='movie_reviews_trainer_image'\n",
    "TAG='latest'\n",
    "TRAINER_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfb4215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 4 file(s) totalling 5.7 KiB before compression.\n",
      "Uploading tarball of [./pipeline/trainer_image] to [gs://dna-verizonpoc_cloudbuild/source/1629299872.666692-b9b32fac77754307a238dddad692a6cd.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/dna-verizonpoc/locations/global/builds/28eb4a67-4d95-49b6-9316-71cd73720ac8].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/28eb4a67-4d95-49b6-9316-71cd73720ac8?project=885855441164].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"28eb4a67-4d95-49b6-9316-71cd73720ac8\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://dna-verizonpoc_cloudbuild/source/1629299872.666692-b9b32fac77754307a238dddad692a6cd.tgz#1629299872908982\n",
      "Copying gs://dna-verizonpoc_cloudbuild/source/1629299872.666692-b9b32fac77754307a238dddad692a6cd.tgz#1629299872908982...\n",
      "/ [1 files][  1.3 KiB/  1.3 KiB]                                                \n",
      "Operation completed over 1 objects/1.3 KiB.                                      \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  10.24kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "feac53061382: Pulling fs layer\n",
      "5f8143275aca: Pulling fs layer\n",
      "9802e64f859b: Pulling fs layer\n",
      "70a7aee60dbd: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "1b5988ba3f15: Pulling fs layer\n",
      "17c28a1e3bb0: Pulling fs layer\n",
      "70a7aee60dbd: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "1b5988ba3f15: Waiting\n",
      "75695f3e4fcc: Pulling fs layer\n",
      "17c28a1e3bb0: Waiting\n",
      "75695f3e4fcc: Waiting\n",
      "e28e443d6ab7: Pulling fs layer\n",
      "7f2c0a4f229b: Pulling fs layer\n",
      "fc7b0c675237: Pulling fs layer\n",
      "9d1427033824: Pulling fs layer\n",
      "c0028679f003: Pulling fs layer\n",
      "09c222e7ff04: Pulling fs layer\n",
      "ae6048a3aec1: Pulling fs layer\n",
      "6284491d0914: Pulling fs layer\n",
      "ce9a50fc8abc: Pulling fs layer\n",
      "e28e443d6ab7: Waiting\n",
      "7f2c0a4f229b: Waiting\n",
      "fc7b0c675237: Waiting\n",
      "9d1427033824: Waiting\n",
      "c0028679f003: Waiting\n",
      "09c222e7ff04: Waiting\n",
      "ae6048a3aec1: Waiting\n",
      "6284491d0914: Waiting\n",
      "ce9a50fc8abc: Waiting\n",
      "5f8143275aca: Verifying Checksum\n",
      "5f8143275aca: Download complete\n",
      "feac53061382: Verifying Checksum\n",
      "feac53061382: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "1b5988ba3f15: Verifying Checksum\n",
      "1b5988ba3f15: Download complete\n",
      "70a7aee60dbd: Verifying Checksum\n",
      "70a7aee60dbd: Download complete\n",
      "75695f3e4fcc: Verifying Checksum\n",
      "75695f3e4fcc: Download complete\n",
      "e28e443d6ab7: Verifying Checksum\n",
      "e28e443d6ab7: Download complete\n",
      "7f2c0a4f229b: Verifying Checksum\n",
      "7f2c0a4f229b: Download complete\n",
      "fc7b0c675237: Verifying Checksum\n",
      "fc7b0c675237: Download complete\n",
      "9d1427033824: Verifying Checksum\n",
      "9d1427033824: Download complete\n",
      "17c28a1e3bb0: Verifying Checksum\n",
      "17c28a1e3bb0: Download complete\n",
      "09c222e7ff04: Verifying Checksum\n",
      "09c222e7ff04: Download complete\n",
      "c0028679f003: Verifying Checksum\n",
      "c0028679f003: Download complete\n",
      "ae6048a3aec1: Download complete\n",
      "ce9a50fc8abc: Verifying Checksum\n",
      "ce9a50fc8abc: Download complete\n",
      "9802e64f859b: Verifying Checksum\n",
      "9802e64f859b: Download complete\n",
      "feac53061382: Pull complete\n",
      "5f8143275aca: Pull complete\n",
      "6284491d0914: Verifying Checksum\n",
      "6284491d0914: Download complete\n",
      "9802e64f859b: Pull complete\n",
      "70a7aee60dbd: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "1b5988ba3f15: Pull complete\n",
      "17c28a1e3bb0: Pull complete\n",
      "75695f3e4fcc: Pull complete\n",
      "e28e443d6ab7: Pull complete\n",
      "7f2c0a4f229b: Pull complete\n",
      "fc7b0c675237: Pull complete\n",
      "9d1427033824: Pull complete\n",
      "c0028679f003: Pull complete\n",
      "09c222e7ff04: Pull complete\n",
      "ae6048a3aec1: Pull complete\n",
      "6284491d0914: Pull complete\n",
      "ce9a50fc8abc: Pull complete\n",
      "Digest: sha256:217efefb946005569e9308e24235bd819eaa802a9fbcb4fd2c030e7b98b3cf92\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> a9df598159fe\n",
      "Step 2/5 : RUN pip install -U fire scikit-learn==0.24.2 pandas==1.3.1 scipy==1.7.0 numpy==1.19.5 joblib==1.0.1\n",
      " ---> Running in 3647146d20f2\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Requirement already satisfied: scikit-learn==0.24.2 in /opt/conda/lib/python3.7/site-packages (0.24.2)\n",
      "Requirement already satisfied: pandas==1.3.1 in /opt/conda/lib/python3.7/site-packages (1.3.1)\n",
      "Requirement already satisfied: scipy==1.7.0 in /opt/conda/lib/python3.7/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy==1.19.5 in /opt/conda/lib/python3.7/site-packages (1.19.5)\n",
      "Requirement already satisfied: joblib==1.0.1 in /opt/conda/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.2) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.1) (2021.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Building wheels for collected packages: fire, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=9e0b2a4f8b720d7d49186ebf5f6edfd908018d0b287d4a363905935014cc59f9\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=28f5b07eb5eba6d7d3f1e600bdca56ffd8ec54fb06e812e98d6b76dc3bbc96e7\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire termcolor\n",
      "Installing collected packages: termcolor, fire\n",
      "Successfully installed fire-0.4.0 termcolor-1.1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 3647146d20f2\n",
      " ---> 2ddaf5570b34\n",
      "Step 3/5 : WORKDIR /app\n",
      " ---> Running in e2acf70eb6f6\n",
      "Removing intermediate container e2acf70eb6f6\n",
      " ---> 845622c40034\n",
      "Step 4/5 : COPY train.py .\n",
      " ---> fa007141c824\n",
      "Step 5/5 : ENTRYPOINT [\"python\", \"train.py\"]\n",
      " ---> Running in 55e214180bea\n",
      "Removing intermediate container 55e214180bea\n",
      " ---> 35aa195f84a4\n",
      "Successfully built 35aa195f84a4\n",
      "Successfully tagged gcr.io/dna-verizonpoc/movie_reviews_trainer_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/dna-verizonpoc/movie_reviews_trainer_image:latest\n",
      "The push refers to repository [gcr.io/dna-verizonpoc/movie_reviews_trainer_image]\n",
      "f77601c6f266: Preparing\n",
      "eeb94e1505c7: Preparing\n",
      "8f55b09a68ac: Preparing\n",
      "3946869edf83: Preparing\n",
      "84569e776a80: Preparing\n",
      "1dccbdf9b557: Preparing\n",
      "cfcbdbc2b748: Preparing\n",
      "937ab8f29c2e: Preparing\n",
      "5d417b2f7486: Preparing\n",
      "da20506c54ba: Preparing\n",
      "2f013dc8d38f: Preparing\n",
      "66e8d7ce97e4: Preparing\n",
      "ef1397e5c777: Preparing\n",
      "32395dfd94f2: Preparing\n",
      "81eaafebd1ec: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "a9b47318f223: Preparing\n",
      "98163d35a813: Preparing\n",
      "262ea1af4c10: Preparing\n",
      "21639b09744f: Preparing\n",
      "1dccbdf9b557: Waiting\n",
      "cfcbdbc2b748: Waiting\n",
      "937ab8f29c2e: Waiting\n",
      "5d417b2f7486: Waiting\n",
      "da20506c54ba: Waiting\n",
      "ef1397e5c777: Waiting\n",
      "2f013dc8d38f: Waiting\n",
      "66e8d7ce97e4: Waiting\n",
      "32395dfd94f2: Waiting\n",
      "81eaafebd1ec: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "a9b47318f223: Waiting\n",
      "98163d35a813: Waiting\n",
      "262ea1af4c10: Waiting\n",
      "21639b09744f: Waiting\n",
      "84569e776a80: Layer already exists\n",
      "3946869edf83: Layer already exists\n",
      "cfcbdbc2b748: Layer already exists\n",
      "1dccbdf9b557: Layer already exists\n",
      "937ab8f29c2e: Layer already exists\n",
      "5d417b2f7486: Layer already exists\n",
      "2f013dc8d38f: Layer already exists\n",
      "da20506c54ba: Layer already exists\n",
      "66e8d7ce97e4: Layer already exists\n",
      "ef1397e5c777: Layer already exists\n",
      "81eaafebd1ec: Layer already exists\n",
      "32395dfd94f2: Layer already exists\n",
      "a9b47318f223: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "98163d35a813: Layer already exists\n",
      "262ea1af4c10: Layer already exists\n",
      "21639b09744f: Layer already exists\n",
      "eeb94e1505c7: Pushed\n",
      "f77601c6f266: Pushed\n",
      "8f55b09a68ac: Pushed\n",
      "latest: digest: sha256:a07b0c1ee718cb864fbf7c356a252110ca897b4d6b2c70705dde019fd850578b size: 4499\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                                       STATUS\n",
      "28eb4a67-4d95-49b6-9316-71cd73720ac8  2021-08-18T15:17:53+00:00  2M28S     gs://dna-verizonpoc_cloudbuild/source/1629299872.666692-b9b32fac77754307a238dddad692a6cd.tgz  gcr.io/dna-verizonpoc/movie_reviews_trainer_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $TRAINER_IMAGE ./pipeline/trainer_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dcff8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='dna-verizonpoc'\n",
    "IMAGE_NAME='movie_reviews_base_image'\n",
    "TAG='latest'\n",
    "BASE_IMAGE='gcr.io/{}/{}:{}'.format(PROJECT_ID, IMAGE_NAME, TAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a463188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 2 file(s) totalling 300 bytes before compression.\n",
      "Uploading tarball of [./pipeline/base_image] to [gs://dna-verizonpoc_cloudbuild/source/1629374755.653616-afc9c1788fff43eaae27079d7ce69e12.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/dna-verizonpoc/locations/global/builds/6c3644c1-bb39-4bf1-a820-774254f45baa].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/6c3644c1-bb39-4bf1-a820-774254f45baa?project=885855441164].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"6c3644c1-bb39-4bf1-a820-774254f45baa\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://dna-verizonpoc_cloudbuild/source/1629374755.653616-afc9c1788fff43eaae27079d7ce69e12.tgz#1629374755880201\n",
      "Copying gs://dna-verizonpoc_cloudbuild/source/1629374755.653616-afc9c1788fff43eaae27079d7ce69e12.tgz#1629374755880201...\n",
      "/ [1 files][  305.0 B/  305.0 B]                                                \n",
      "Operation completed over 1 objects/305.0 B.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  3.584kB\n",
      "Step 1/2 : FROM gcr.io/deeplearning-platform-release/base-cpu\n",
      "latest: Pulling from deeplearning-platform-release/base-cpu\n",
      "feac53061382: Pulling fs layer\n",
      "a6f1b3ffe450: Pulling fs layer\n",
      "94bdb55a8dd4: Pulling fs layer\n",
      "9955636061ee: Pulling fs layer\n",
      "4f4fb700ef54: Pulling fs layer\n",
      "e022b8a62d1d: Pulling fs layer\n",
      "328f21c1ca9f: Pulling fs layer\n",
      "2e054d8bd926: Pulling fs layer\n",
      "4d233af94ed9: Pulling fs layer\n",
      "2cde7b4ff712: Pulling fs layer\n",
      "801bc4033f73: Pulling fs layer\n",
      "f6bd7e46914f: Pulling fs layer\n",
      "f2e085a6dbe1: Pulling fs layer\n",
      "596036b1d447: Pulling fs layer\n",
      "73691ce4acee: Pulling fs layer\n",
      "ecb664918de3: Pulling fs layer\n",
      "fdb1e1e79006: Pulling fs layer\n",
      "9955636061ee: Waiting\n",
      "2cde7b4ff712: Waiting\n",
      "4f4fb700ef54: Waiting\n",
      "e022b8a62d1d: Waiting\n",
      "801bc4033f73: Waiting\n",
      "f6bd7e46914f: Waiting\n",
      "f2e085a6dbe1: Waiting\n",
      "596036b1d447: Waiting\n",
      "73691ce4acee: Waiting\n",
      "328f21c1ca9f: Waiting\n",
      "2e054d8bd926: Waiting\n",
      "4d233af94ed9: Waiting\n",
      "ecb664918de3: Waiting\n",
      "fdb1e1e79006: Waiting\n",
      "a6f1b3ffe450: Verifying Checksum\n",
      "a6f1b3ffe450: Download complete\n",
      "feac53061382: Verifying Checksum\n",
      "feac53061382: Download complete\n",
      "4f4fb700ef54: Verifying Checksum\n",
      "4f4fb700ef54: Download complete\n",
      "e022b8a62d1d: Verifying Checksum\n",
      "e022b8a62d1d: Download complete\n",
      "9955636061ee: Verifying Checksum\n",
      "9955636061ee: Download complete\n",
      "2e054d8bd926: Verifying Checksum\n",
      "2e054d8bd926: Download complete\n",
      "4d233af94ed9: Verifying Checksum\n",
      "4d233af94ed9: Download complete\n",
      "2cde7b4ff712: Verifying Checksum\n",
      "2cde7b4ff712: Download complete\n",
      "801bc4033f73: Verifying Checksum\n",
      "801bc4033f73: Download complete\n",
      "f6bd7e46914f: Download complete\n",
      "f2e085a6dbe1: Verifying Checksum\n",
      "f2e085a6dbe1: Download complete\n",
      "596036b1d447: Verifying Checksum\n",
      "596036b1d447: Download complete\n",
      "73691ce4acee: Verifying Checksum\n",
      "73691ce4acee: Download complete\n",
      "328f21c1ca9f: Verifying Checksum\n",
      "328f21c1ca9f: Download complete\n",
      "fdb1e1e79006: Verifying Checksum\n",
      "fdb1e1e79006: Download complete\n",
      "94bdb55a8dd4: Verifying Checksum\n",
      "94bdb55a8dd4: Download complete\n",
      "feac53061382: Pull complete\n",
      "a6f1b3ffe450: Pull complete\n",
      "ecb664918de3: Verifying Checksum\n",
      "ecb664918de3: Download complete\n",
      "94bdb55a8dd4: Pull complete\n",
      "9955636061ee: Pull complete\n",
      "4f4fb700ef54: Pull complete\n",
      "e022b8a62d1d: Pull complete\n",
      "328f21c1ca9f: Pull complete\n",
      "2e054d8bd926: Pull complete\n",
      "4d233af94ed9: Pull complete\n",
      "2cde7b4ff712: Pull complete\n",
      "801bc4033f73: Pull complete\n",
      "f6bd7e46914f: Pull complete\n",
      "f2e085a6dbe1: Pull complete\n",
      "596036b1d447: Pull complete\n",
      "73691ce4acee: Pull complete\n",
      "ecb664918de3: Pull complete\n",
      "fdb1e1e79006: Pull complete\n",
      "Digest: sha256:8452c62f189698513d86d939098493323cba63a4f31797ce82ab67ec97b19b94\n",
      "Status: Downloaded newer image for gcr.io/deeplearning-platform-release/base-cpu:latest\n",
      " ---> 6997da3042b2\n",
      "Step 2/2 : RUN pip install -U fire scikit-learn==0.24.2 pandas==1.3.1 scipy==1.7.0 numpy==1.19.5 joblib==1.0.1\n",
      " ---> Running in 648e4c231ddd\n",
      "Collecting fire\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "Requirement already satisfied: scikit-learn==0.24.2 in /opt/conda/lib/python3.7/site-packages (0.24.2)\n",
      "Collecting pandas==1.3.1\n",
      "  Downloading pandas-1.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "Collecting scipy==1.7.0\n",
      "  Downloading scipy-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
      "Requirement already satisfied: numpy==1.19.5 in /opt/conda/lib/python3.7/site-packages (1.19.5)\n",
      "Requirement already satisfied: joblib==1.0.1 in /opt/conda/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.2) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas==1.3.1) (2021.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from fire) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Building wheels for collected packages: fire, termcolor\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=ececcb8d31b3fb3e6e579020dec4e2593fdafe35e23431dc1ca248e9993dbda1\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4847 sha256=1402c7e3a1bc560cbf7f666148a21d425b63340ab7e9a0e0eddec44afbe1400b\n",
      "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built fire termcolor\n",
      "Installing collected packages: termcolor, scipy, pandas, fire\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.7.1\n",
      "    Uninstalling scipy-1.7.1:\n",
      "      Successfully uninstalled scipy-1.7.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.2\n",
      "    Uninstalling pandas-1.3.2:\n",
      "      Successfully uninstalled pandas-1.3.2\n",
      "Successfully installed fire-0.4.0 pandas-1.3.1 scipy-1.7.0 termcolor-1.1.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 648e4c231ddd\n",
      " ---> 8c444b6885b2\n",
      "Successfully built 8c444b6885b2\n",
      "Successfully tagged gcr.io/dna-verizonpoc/movie_reviews_base_image:latest\n",
      "PUSH\n",
      "Pushing gcr.io/dna-verizonpoc/movie_reviews_base_image:latest\n",
      "The push refers to repository [gcr.io/dna-verizonpoc/movie_reviews_base_image]\n",
      "b07eb7554ef6: Preparing\n",
      "3a2b1d889075: Preparing\n",
      "841fd27a33c3: Preparing\n",
      "fa1d43a0b8a6: Preparing\n",
      "21b8e5e33f1e: Preparing\n",
      "9cc58e8f4996: Preparing\n",
      "671bd3afbbd2: Preparing\n",
      "abdd3a0d9161: Preparing\n",
      "fa2e27f9bc83: Preparing\n",
      "2b67839b703f: Preparing\n",
      "c999038361c3: Preparing\n",
      "c04972ab02bc: Preparing\n",
      "45b0f59ba4e4: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "eb9a178bb296: Preparing\n",
      "478982d95108: Preparing\n",
      "02c197c8aec4: Preparing\n",
      "21639b09744f: Preparing\n",
      "2b67839b703f: Waiting\n",
      "c999038361c3: Waiting\n",
      "c04972ab02bc: Waiting\n",
      "45b0f59ba4e4: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "9cc58e8f4996: Waiting\n",
      "671bd3afbbd2: Waiting\n",
      "abdd3a0d9161: Waiting\n",
      "eb9a178bb296: Waiting\n",
      "478982d95108: Waiting\n",
      "fa2e27f9bc83: Waiting\n",
      "02c197c8aec4: Waiting\n",
      "21639b09744f: Waiting\n",
      "21b8e5e33f1e: Mounted from deeplearning-platform-release/base-cpu\n",
      "fa1d43a0b8a6: Mounted from deeplearning-platform-release/base-cpu\n",
      "3a2b1d889075: Mounted from deeplearning-platform-release/base-cpu\n",
      "841fd27a33c3: Mounted from deeplearning-platform-release/base-cpu\n",
      "9cc58e8f4996: Mounted from deeplearning-platform-release/base-cpu\n",
      "671bd3afbbd2: Mounted from deeplearning-platform-release/base-cpu\n",
      "abdd3a0d9161: Mounted from deeplearning-platform-release/base-cpu\n",
      "fa2e27f9bc83: Mounted from deeplearning-platform-release/base-cpu\n",
      "2b67839b703f: Mounted from deeplearning-platform-release/base-cpu\n",
      "5f70bf18a086: Layer already exists\n",
      "c999038361c3: Mounted from deeplearning-platform-release/base-cpu\n",
      "c04972ab02bc: Mounted from deeplearning-platform-release/base-cpu\n",
      "45b0f59ba4e4: Mounted from deeplearning-platform-release/base-cpu\n",
      "eb9a178bb296: Mounted from deeplearning-platform-release/base-cpu\n",
      "478982d95108: Mounted from deeplearning-platform-release/base-cpu\n",
      "21639b09744f: Layer already exists\n",
      "02c197c8aec4: Mounted from deeplearning-platform-release/base-cpu\n",
      "b07eb7554ef6: Pushed\n",
      "latest: digest: sha256:4b2913ba91a28377f22a42346eb97fe8487b1db9fc39243dea4a97926b8ed04f size: 4086\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                        IMAGES                                                    STATUS\n",
      "6c3644c1-bb39-4bf1-a820-774254f45baa  2021-08-19T12:05:56+00:00  2M57S     gs://dna-verizonpoc_cloudbuild/source/1629374755.653616-afc9c1788fff43eaae27079d7ce69e12.tgz  gcr.io/dna-verizonpoc/movie_reviews_base_image (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --timeout 15m --tag $BASE_IMAGE ./pipeline/base_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d31aaad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0dbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.google import experimental\n",
    "from kfp.v2.google.client import AIPlatformClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create args list for trainer\n",
    "\n",
    "PIPELINE_ROOT=\"gs://verexai_automl_text_data/pipeline_root/custommodel/imdb_reviews/\"\n",
    "TRAINER_ARGS = [\"gs://verexai_automl_text_data/pipeline_root/custommodel/imdb_reviews/data/imdb_train.csv\", \"gs://verexai_automl_text_data/pipeline_root/custommodel/imdb_reviews/data/imdb_test.csv\", \"gs://verexai_automl_text_data/pipeline_root/custommodel/imdb_reviews/model/\"]\n",
    "\n",
    "# create working dir to pass to job spec\n",
    "import time\n",
    "\n",
    "ts = int(time.time())\n",
    "WORKING_DIR = f\"{PIPELINE_ROOT}/{ts}\"\n",
    "\n",
    "MODEL_DISPLAY_NAME = f\"movie-reviews-sentiment-anlaysis{ts}\"\n",
    "print(TRAINER_ARGS, WORKING_DIR, MODEL_DISPLAY_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component\n",
    "def training_op(input1: str):\n",
    "    print(\"training task: {}\".format(input1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d227ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"movie-reviews-sentiment-anlaysis\" + TIMESTAMP)\n",
    "def pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    model_display_name: str = MODEL_DISPLAY_NAME,\n",
    "    serving_container_image_uri: str = \"us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-3:latest\",\n",
    "):\n",
    "\n",
    "    train_task = training_op(\"model training\")\n",
    "    experimental.run_as_aiplatform_custom_job(\n",
    "        train_task,\n",
    "        worker_pool_specs=[\n",
    "            {\n",
    "                \"containerSpec\": {\n",
    "                    \"args\": TRAINER_ARGS,\n",
    "                    \"env\": [{\"name\": \"AIP_MODEL_DIR\", \"value\": WORKING_DIR}],\n",
    "                    \"imageUri\": \"gcr.io/dna-verizonpoc/movie_reviews_trainer_image@sha256:a07b0c1ee718cb864fbf7c356a252110ca897b4d6b2c70705dde019fd850578b\",\n",
    "                },\n",
    "                \"replicaCount\": \"1\",\n",
    "                \"machineSpec\": {\n",
    "                    \"machineType\": \"n1-standard-4\",\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    model_upload_op = gcc_aip.ModelUploadOp(\n",
    "        project=project,\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=WORKING_DIR,\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        serving_container_environment_variables={\"NOT_USED\": \"NO_VALUE\"},\n",
    "    )\n",
    "    model_upload_op.after(train_task)\n",
    "\n",
    "    endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name=\"pipelines-created-endpoint\",\n",
    "    )\n",
    "\n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(  # noqa: F841\n",
    "        project=project,\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=model_display_name,\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec52b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id=\"dna-verizonpoc\"\n",
    "region=\"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38fe817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component)\n",
    "\n",
    "from kfp.v2 import compiler\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8fff4415",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component( base_image=\"gcr.io/dna-verizonpoc/movie_reviews_base_image:latest\")\n",
    "def get_data(\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset]\n",
    "    \n",
    "):\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "    import pandas as pd\n",
    "    # import some data to play with\n",
    "    \n",
    "    \n",
    "    data_raw = pd.read_csv(\"gs://verexai_automl_text_data/pipeline_root/custommodel/imdb_reviews/data/imdb_movie_reviews_data.csv\")\n",
    "    train, test = tts(data_raw, test_size=0.3)\n",
    "    \n",
    "    train.to_csv(dataset_train.path)\n",
    "    test.to_csv(dataset_test.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a81d1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "        \"joblib\",\n",
    "        \"scipy\"\n",
    "    ],\n",
    ")\n",
    "def train_movie_reviews_model(\n",
    "    dataset: Input[Dataset],\n",
    "    model_artifact: Output[Model]\n",
    "):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    from os import system\n",
    "    import subprocess\n",
    "    import sys\n",
    "    import fire\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "    from joblib import dump, load # used for saving and loading sklearn objects\n",
    "    from scipy.sparse import save_npz, load_npz, csr_matrix # used for saving and loading sparse matrices\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from scipy.stats import uniform\n",
    "    \n",
    "    imdb_train = pd.read_csv(dataset.path)\n",
    "    \n",
    "    if not os.path.exists('model'):\n",
    "        system(\"mkdir 'model'\")\n",
    "\n",
    "    #preprocessing\n",
    "\n",
    "    # Bigram Counts\n",
    "\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    bigram_vectorizer.fit(imdb_train['text'].values)\n",
    "    \n",
    "    X_train_bigram = bigram_vectorizer.transform(imdb_train['text'].values)\n",
    "\n",
    "    # Bigram Tf-Idf\n",
    "\n",
    "    bigram_tf_idf_transformer = TfidfTransformer()\n",
    "    bigram_tf_idf_transformer.fit(X_train_bigram)\n",
    "    X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\n",
    "    y_train = imdb_train['label'].values\n",
    "\n",
    "    def train_and_show_scores(X_train: csr_matrix, y_train: np.array, title: str) -> None:\n",
    "        \n",
    "        clf = SGDClassifier()\n",
    "\n",
    "        distributions = dict(\n",
    "            loss=['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "            learning_rate=['optimal', 'invscaling', 'adaptive'],\n",
    "            eta0=uniform(loc=1e-7, scale=1e-2)\n",
    "        )\n",
    "\n",
    "        random_search_cv = RandomizedSearchCV(\n",
    "            estimator=clf,\n",
    "            param_distributions=distributions,\n",
    "            cv=5,\n",
    "            n_iter=20\n",
    "        )\n",
    "        random_search_cv.fit(X_train, y_train)\n",
    "        print(f'Best params: {random_search_cv.best_params_}')\n",
    "        print(f'Best score: {random_search_cv.best_score_}')\n",
    "        dump(random_search_cv.best_estimator_,'model/movie_reviews_model.joblib.pkl')\n",
    "        dump('model/movie_reviews_model.joblib.pkl',model_artifact.path)\n",
    "        model_artifact.metadata[\"best_params\"] = random_search_cv.best_params_\n",
    "        model_artifact.metadata[\"train_score\"] = float(random_search_cv.best_score_)\n",
    "        model_artifact.metadata[\"framework\"] = \"Scikit-learn\"\n",
    "\n",
    "    train_and_show_scores(X_train_bigram_tf_idf, y_train, 'Bigram Tf-Idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "668294ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install = [\n",
    "        \"pandas\",\n",
    "        \"sklearn\",\n",
    "        \"joblib\"\n",
    "    ],\n",
    ")\n",
    "def eval_model(\n",
    "    test_set: Input[Dataset],\n",
    "    movie_review_model: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    smetrics: Output[Metrics]\n",
    "):\n",
    "    import pandas as pd\n",
    "    from joblib import dump, load\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "    imdb_test = pd.read_csv(test_set.path)\n",
    "    bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    bigram_tf_idf_transformer = TfidfTransformer()\n",
    "    X_test = bigram_vectorizer.transform(imdb_test['text'].values)\n",
    "    X_test = bigram_tf_idf_transformer.transform(X_test)\n",
    "    y_test = imdb_test['label'].values\n",
    "    model = load(movie_review_model.path)\n",
    "    score = model.score(X_test, y_test)\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    y_pred = model.predict(imdb_test['text'].values)\n",
    "    \n",
    "    metrics.log_confusion_matrix(\n",
    "       [\"False\", \"True\"],\n",
    "       confusion_matrix(\n",
    "           imdb_test['label'].values, y_pred\n",
    "       ).tolist(),  # .tolist() to convert np array to list.\n",
    "    )\n",
    "    \n",
    "    movie_review_model.metadata[\"test_score\"] = float(score)\n",
    "    smetrics.log_metric(\"score\", float(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096cfae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "740d01a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    # Default pipeline root. You can override it when submitting the pipeline.\n",
    "    pipeline_root=\"gs://verexai_automl_text_data/pipeline_root/custommodel/imdb_reviews/\",\n",
    "    # A name for the pipeline. Use to determine the pipeline Context.\n",
    "    name=\"custom-pipeline-text-sentiment-anlaysis\",\n",
    ")\n",
    "def pipeline():\n",
    "    dataset_op = get_data()\n",
    "    train_op = train_movie_reviews_model(dataset_op.outputs[\"dataset_train\"])\n",
    "    eval_op = eval_model(\n",
    "        test_set=dataset_op.outputs[\"dataset_test\"],\n",
    "        movie_review_model=train_op.outputs[\"model_artifact\"]\n",
    "    )\n",
    "    endpoint_create_op = gcc_aip.EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name=\"pipelines-created-endpoint-text-sa\",\n",
    "    )\n",
    "\n",
    "    model_deploy_op = gcc_aip.ModelDeployOp(         \n",
    "        project=project_id,\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=train_op.outputs[\"model_artifact\"],\n",
    "        deployed_model_display_name=\"movie_reviews_model\",\n",
    "        machine_type=\"n1-standard-4\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='movie_review_pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea62f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model_with_dedicated_resources_sample(\n",
    "    project,\n",
    "    location,\n",
    "    movie_review_model: Input[Model],\n",
    "    machine_type: str,\n",
    "    endpoint: Input[Endpoint],\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    sync: bool = True,\n",
    "):\n",
    "\n",
    "    gcc_aip.init(project=project_id, location=region)\n",
    "\n",
    "    model = aiplatform.Model(model_name=\"movie_reviews_model\")\n",
    "\n",
    "    # The explanation_metadata and explanation_parameters should only be\n",
    "    # provided for a custom trained model and not an AutoML model.\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=\"movie_review_model\",\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    model.wait()\n",
    "\n",
    "    print(model.display_name)\n",
    "    print(model.resource_name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5decda6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-pipeline-text-sentiment-anlaysis-20210820065203?project=dna-verizonpoc\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "api_client = AIPlatformClient(\n",
    "                project_id=\"dna-verizonpoc\",\n",
    "                region=\"us-central1\"\n",
    "                )\n",
    "\n",
    "response = api_client.create_run_from_job_spec(\n",
    "    'movie_review_pipeline.json',\n",
    "    enable_caching=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3c247213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composing gs://verexai_automl_text_data/pipeline_root/custommodel/imdb_reviews/data/imdb_movie_reviews_data.csv from 2 component object(s).\n"
     ]
    }
   ],
   "source": [
    "!gsutil compose gs://verexai_automl_text_data/pipeline_root/custommodel/imdb_reviews/data/imdb_train.csv gs://verexai_automl_text_data/pipeline_root/custommodel/imdb_reviews/data/imdb_test.csv gs://verexai_automl_text_data/pipeline_root/custommodel/imdb_reviews/data/imdb_movie_reviews_data.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "774a6070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Throw this lame dog a bone. Sooo bad...you may...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This film limps from self indulgent moment to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The story is very trustworthy and powerful. Th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I enjoy movies like this for their spirit, no ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I wondered why John Wood was not playing Dr. F...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  Throw this lame dog a bone. Sooo bad...you may...     0\n",
       "1  This film limps from self indulgent moment to ...     0\n",
       "2  The story is very trustworthy and powerful. Th...     0\n",
       "3  I enjoy movies like this for their spirit, no ...     1\n",
       "4  I wondered why John Wood was not playing Dr. F...     0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_raw = pd.read_csv(\"gs://verexai_automl_text_data/pipeline_root/custommodel/imdb_reviews/data/imdb_movie_reviews_data.csv\")\n",
    "data_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f953298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/Vertexai-Custom-Model-Covertype/Imdb_reviews_custom\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51d21fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233065f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
